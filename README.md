# ü¶ô OpenLLM on Google Cloud VM (NVIDIA L4)

This project lets you deploy an OpenLLM server on a Google Compute Engine VM with GPU support (NVIDIA L4).

---

## üöÄ How it works

- Provisions a VM with NVIDIA GPU (e.g., L4)
- Installs drivers + CUDA + Python venv + OpenLLM
- Uses `systemd` to run OpenLLM at boot
- Exposes the `/chat` UI on your public IP

---

## üóÇÔ∏è Files in this repo

| File | Purpose |
|------|---------|
| `startup.sh` | Run this on a fresh VM to install all dependencies |
| `openllm.service` | Example systemd unit file to run OpenLLM automatically |
| `benchmark.py` | Script to test completions and tokens/sec |

---

## ‚úÖ Steps to use

1. **Provision your VM**

   Use an L4 (or A100) machine with enough RAM and VRAM.

2. **Run the setup**

   ```bash
   chmod +x startup.sh
   ./startup.sh
   ```

3. **Edit & copy `openllm.service`**

   ```bash
   sudo cp openllm.service /etc/systemd/system/openllm.service
   sudo systemctl daemon-reload
   sudo systemctl enable openllm
   sudo systemctl start openllm
   ```

   Update the `ExecStart` model to your choice!

4. **Test**

   - Open http://YOUR_EXTERNAL_IP/chat
   - Or call the API:
     ```bash
     curl http://127.0.0.1:3000/v1/completions        -H "Content-Type: application/json"        -d '{"prompt": "Say hi", "max_tokens": 20}'
     ```

5. **Benchmark**

   ```bash
   python3 benchmark.py
   ```

6. **Stop & start**

   ```bash
   gcloud compute instances stop YOUR_VM
   gcloud compute instances start YOUR_VM
   ```

---

## üöÄ Launch on GCP

[![Open in Cloud Shell](https://gstatic.com/cloudssh/images/open-btn.png)](https://console.cloud.google.com/cloudshell/open?git_repo=https://github.com/thecloudranger/openllm-gcp-vm&cloudshell_tutorial=README.md)

---

## ‚ö° Tips

- Reserve a static IP if you want a fixed public endpoint.
- Watch GPU usage: `watch -n 1 nvidia-smi`.
- Tune your model in `ExecStart` or switch it later.

---

## üìú License

MIT ‚Äî do whatever you want!
